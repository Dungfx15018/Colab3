{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llc17e6ao5zZ"
   },
   "source": [
    "## Lựa chọn đặc trưng bằng Xáo trộn ngẫu nhiên (Random Shuffling)\n",
    "\n",
    "Đây là phương pháp lựa chọn đặc trưng phổ biến, xáo trộn ngẫu nhiên các giá trị của một biến cụ thể và xác định xem phép hoán vị đó ảnh hưởng như thế nào đến chỉ số chất lượng của thuật toán học máy. Nói cách khác, ý tưởng ở đây là hoán vị các giá trị của từng đặc trưng, đo mức độ hoán vị (hoặc xáo trộn các giá trị của nó) làm giảm accuracy (độ chính xác), roc_auc hoặc mse của mô hình học máy (hoặc bất kỳ số liệu chất lượng nào khác!). Nếu các biến quan trọng, hoán vị ngẫu nhiên các giá trị sẽ giảm đáng kể bất kỳ số liệu nào trong số này. Ngược lại, hoán vị hoặc xáo trộn các giá trị sẽ ít hoặc không ảnh hưởng đến chỉ số chất lượng của mô hình mà chúng ta đang đánh giá.\n",
    "\n",
    "Quy trình như sau:\n",
    "\n",
    "- Xây dựng mô hình học máy và lưu trữ chỉ số chất lượng\n",
    "- Xáo trộn một đặc trưng và đưa ra dự đoán mới sử dụng mô hình trước đó\n",
    "- Xác định chất lượng của dự đoán này\n",
    "- Xác định thay đổi trong chất lượng của dự đoán với đặc trưng đa xáo trộn so với đặc trưng ban đầu\n",
    "- Lặp lại cho từng đặc trưng\n",
    "\n",
    "Chúng ta sẽ chọn các đặc trưng làm giảm chất lượng mô hình vượt một ngưỡng thiết lập bất kỳ để lựa chọn đặc trưng.\n",
    "\n",
    "Chúng ta sẽ minh họa cách lựa chọn đặc trưng dựa trên xáo trộn ngẫu nhiên trong bài toán hồi quy và phân loại.\n",
    "\n",
    "**Lưu ý** Chúng ta sẽ tiếp tục dùng Random Forest nhưng với quy trình lựa chọn này, có thể sử dụng với cả thuật toán học máy. Trên thực tế, cần xác định rành mạch độ quan trọng của các đặc trưng cho thuật toán được dùng. Do đó, các thuật toán khác nhau sẽ trả về các tập hợp con khác nhau của đặc trưng quan trọng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BHrSh_-co5zc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XKuqsx-o5zc"
   },
   "source": [
    "## Phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "utOlab-Zo5zd",
    "outputId": "2b5bf3d8-7dc9-4638-b010-c3108208d751"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 109)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tập dữ liệu\n",
    "data = pd.read_csv('dataset_2.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wqLLhE__o5ze",
    "outputId": "8ac63dbe-1981-4e8c-9d20-c576600dd563"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532710</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.349910</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.821374</td>\n",
       "      <td>12.098722</td>\n",
       "      <td>13.309151</td>\n",
       "      <td>4.125599</td>\n",
       "      <td>1.045386</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>8.652883</td>\n",
       "      <td>0.102757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479789</td>\n",
       "      <td>7.795290</td>\n",
       "      <td>3.557890</td>\n",
       "      <td>17.383378</td>\n",
       "      <td>15.193423</td>\n",
       "      <td>8.263673</td>\n",
       "      <td>1.878108</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>1.018818</td>\n",
       "      <td>1.416433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.938776</td>\n",
       "      <td>7.952752</td>\n",
       "      <td>0.972671</td>\n",
       "      <td>3.459267</td>\n",
       "      <td>1.935782</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>2.338139</td>\n",
       "      <td>0.344948</td>\n",
       "      <td>9.937850</td>\n",
       "      <td>11.691283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861487</td>\n",
       "      <td>6.130886</td>\n",
       "      <td>3.401064</td>\n",
       "      <td>15.850471</td>\n",
       "      <td>14.620599</td>\n",
       "      <td>6.849776</td>\n",
       "      <td>1.098210</td>\n",
       "      <td>1.959183</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>1.857893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.020690</td>\n",
       "      <td>9.900544</td>\n",
       "      <td>17.869637</td>\n",
       "      <td>4.366715</td>\n",
       "      <td>1.973693</td>\n",
       "      <td>2.026012</td>\n",
       "      <td>2.853025</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>11.816859</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340944</td>\n",
       "      <td>7.240058</td>\n",
       "      <td>2.417235</td>\n",
       "      <td>15.194609</td>\n",
       "      <td>13.553772</td>\n",
       "      <td>7.229971</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>2.234482</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>2.700606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.909506</td>\n",
       "      <td>10.576516</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>3.419572</td>\n",
       "      <td>1.871438</td>\n",
       "      <td>3.340811</td>\n",
       "      <td>1.868282</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>13.585620</td>\n",
       "      <td>1.153366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.738095</td>\n",
       "      <td>6.565509</td>\n",
       "      <td>4.341414</td>\n",
       "      <td>15.893832</td>\n",
       "      <td>11.929787</td>\n",
       "      <td>6.954033</td>\n",
       "      <td>1.853364</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>2.599562</td>\n",
       "      <td>0.811364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_1      var_2      var_3     var_4     var_5     var_6     var_7  \\\n",
       "0  4.532710   3.280834  17.982476  4.404259  2.349910  0.603264  2.784655   \n",
       "1  5.821374  12.098722  13.309151  4.125599  1.045386  1.832035  1.833494   \n",
       "2  1.938776   7.952752   0.972671  3.459267  1.935782  0.621463  2.338139   \n",
       "3  6.020690   9.900544  17.869637  4.366715  1.973693  2.026012  2.853025   \n",
       "4  3.909506  10.576516   0.934191  3.419572  1.871438  3.340811  1.868282   \n",
       "\n",
       "      var_8      var_9     var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691   0.139346  ...  2.079066  6.748819  2.941445   \n",
       "1  0.709090   8.652883   0.102757  ...  2.479789  7.795290  3.557890   \n",
       "2  0.344948   9.937850  11.691283  ...  1.861487  6.130886  3.401064   \n",
       "3  0.674847  11.816859   0.011151  ...  1.340944  7.240058  2.417235   \n",
       "4  0.439865  13.585620   1.153366  ...  2.738095  6.565509  4.341414   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "1  17.383378  15.193423  8.263673  1.878108  0.567939  1.018818  1.416433  \n",
       "2  15.850471  14.620599  6.849776  1.098210  1.959183  1.575493  1.857893  \n",
       "3  15.194609  13.553772  7.229971  0.835158  2.234482  0.946170  2.700606  \n",
       "4  15.893832  11.929787  6.954033  1.853364  0.511027  2.599562  0.811364  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h7-nfH0o5zf"
   },
   "source": [
    "**Quan trọng**\n",
    "\n",
    "Trong tất cả các quy trình lựa chọn đặc trưng, chỉ nên chọn các đặc trưng bằng cách kiểm tra tập huấn luyện, điều này giúp tránh overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fnS4BRFuo5zf",
    "outputId": "7576bb50-a857-484f-fb69-c54dded58b04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tách thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QO1VLZHVo5zh"
   },
   "outputs": [],
   "source": [
    "# với phương thức này, cần đặt lại chỉ số của\n",
    "# các tập dữ liệu được trả về\n",
    "## Yêu cầu 1:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4lZ6MIsq9GN"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "[reset_index()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdymw15So5zi"
   },
   "source": [
    "### Huấn luyện thuật toán học máy với toàn bộ các đặc trưng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "owVq7aPio5zj",
    "outputId": "5d501444-1c22-4395-8221-162a686b64b2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc score:  0.690997114685582\n",
      "test auc score:  0.6857035229040285\n"
     ]
    }
   ],
   "source": [
    "# Bước đầu tiên là xác định độ quan trọng của đặc trưng bằng cách xáo trộn đặc trưng\n",
    "# để xây dựng mô hình học máy mà chúng ta cần \n",
    "# để lựa chọn đặc trưng\n",
    "\n",
    "# Ở trường hợp này, chúng ta sẽ xây dựng Random Forest, nhưng hãy nhớ rằng \n",
    "# có thể dùng quy trình này với bất kỳ thuật tóan học máy nào khác\n",
    "\n",
    "## Yêu cầu 2:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "# xây dựng ít cây và cây cần nông (shallow) để tránh overfitting\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50, max_depth=2, random_state=2909, n_jobs=4)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# in ra roc-auc trên các tập huấn luyện và tập kiểm tra\n",
    "print('train auc score: ',\n",
    "      roc_auc_score(y_train, (rf.predict_proba(X_train.fillna(0)))[:, 1]))\n",
    "print('test auc score: ',\n",
    "      roc_auc_score(y_test, (rf.predict_proba(X_test.fillna(0)))[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQQLTVqcxE2F"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "[RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "[roc_auc_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5HPErRgo5zj"
   },
   "source": [
    "### Xáo trộn đặc trưng và đánh giá mức giảm chất lượng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Hfj_6L43o5zk"
   },
   "outputs": [],
   "source": [
    "# trong cell này, hãy xáo trộn lần lượt từng đặc trưng của tập dữ liệu\n",
    "\n",
    "# sau đó sử dụng tập dữ liệu có biến đã xáo trộn để đưa ra dự đoán \n",
    "# với các random forest đã huấn luyện ở cell trước\n",
    "\n",
    "# train roc-auc tổng quát: dùng toàn bộ đặc trưng\n",
    "train_roc = roc_auc_score(y_train, (rf.predict_proba(X_train))[:, 1])\n",
    "\n",
    "# danh sách chứa các thay đổi về chất lượng\n",
    "performance_shift = []\n",
    "\n",
    "# logic lựa chọn\n",
    "for feature in X_train.columns:\n",
    "\n",
    "    X_train_c = X_train.copy()\n",
    "\n",
    "    # xáo trộn đặc trưng riêng lẻ\n",
    "    X_train_c[feature] = X_train_c[feature].sample(\n",
    "        frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "    # dự đoán với đặc trưng đã xáo trộn và tính roc-auc\n",
    "    shuff_roc = roc_auc_score(y_train, rf.predict_proba(X_train_c)[:, 1])\n",
    "    \n",
    "    drift = train_roc - shuff_roc\n",
    "\n",
    "    # lưu mức giảm trong roc-auc\n",
    "    performance_shift.append(drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tJSyg1VBo5zk",
    "outputId": "d80e2778-bd05-4edb-ee4c-64032055cb82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " -9.919140466640997e-05,\n",
       " -5.777064524881137e-05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -3.334693591705573e-05,\n",
       " 8.796265542265758e-05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.2864223244933868e-05,\n",
       " -6.957465160806198e-05,\n",
       " 4.371055238927557e-05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.015497219959881292,\n",
       " -0.00012937667354617766,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0013551709383483601,\n",
       " 0.0,\n",
       " -7.38081844428029e-05,\n",
       " 0.0,\n",
       " 1.2296120844856873e-05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0015281413151823076,\n",
       " 2.6043867067171433e-05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.001336418904640313,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.00015224539098712686,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.020099856532177e-06,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.17743806627535e-05,\n",
       " 0.0,\n",
       " 0.0017028464517550024,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07156079421237771,\n",
       " 0.0,\n",
       " -0.00015256447891842662,\n",
       " 4.3209449511305564e-05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.00018709114134207727,\n",
       " -8.323251390618402e-05,\n",
       " 0.00010579113180830824,\n",
       " 1.5033086339877322e-05,\n",
       " 0.0,\n",
       " 3.216945650863501e-05,\n",
       " 0.008743101448133728,\n",
       " 0.0035736702283486466,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.00010268788932177308,\n",
       " 8.200559833926313e-05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.2970250277133388e-05,\n",
       " 0.0,\n",
       " 0.0002834152488229158,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0003512281755146951,\n",
       " 0.0,\n",
       " -0.00023584867608106297,\n",
       " 0.0,\n",
       " 0.0006831133305189585,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0010592460943602555,\n",
       " 4.597338018363928e-05,\n",
       " -4.512173000126296e-06,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0003018279707167615,\n",
       " -5.0818123703777474e-05,\n",
       " 0.0003170565545920212,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.00023036126250230993,\n",
       " 0.0,\n",
       " 2.3549588167304236e-06,\n",
       " 3.656702750509666e-05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0066017899781302125,\n",
       " 0.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hãy xem danh sách các chất lượng\n",
    "performance_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "G2bmTonzo5zl",
    "outputId": "36c7b504-d32b-4ff7-f71e-e0fbac15dcce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var_1    0.000000\n",
       "var_2   -0.000099\n",
       "var_3   -0.000058\n",
       "var_4    0.000000\n",
       "var_5    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hãy biến đổi list (danh sách) thành pandas Series\n",
    "# để dễ thao tác\n",
    "\n",
    "feature_importance = pd.Series(performance_shift)\n",
    "\n",
    "# thêm tên biến vào chỉ số\n",
    "feature_importance.index = X_train.columns\n",
    "\n",
    "feature_importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_E_4YwdEo5zl",
    "outputId": "2381324c-c469-47e9-c507-8f6873a4fd9e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var_55     0.071561\n",
       "var_16     0.015497\n",
       "var_69     0.008743\n",
       "var_108    0.006602\n",
       "var_70     0.003574\n",
       "             ...   \n",
       "var_63    -0.000187\n",
       "var_102   -0.000230\n",
       "var_86    -0.000236\n",
       "var_84    -0.000351\n",
       "var_30    -0.001528\n",
       "Length: 108, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Giờ hãy sắp xếp dataframe theo mức giảm chất lượng\n",
    "# do xáo trộn đặc trưng\n",
    "## Yêu cầu 3:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "feature_importance.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoYcbNsOrTar"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "[sort_values()](https://pandas.pydata.org/docs/reference/api/pandas.Series.sort_values.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WJnqGM28o5zm",
    "outputId": "5c95a15c-e7fd-4e3c-b1d6-9da11535605c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var_55     0.071561\n",
       "var_16     0.015497\n",
       "var_69     0.008743\n",
       "var_108    0.006602\n",
       "var_70     0.003574\n",
       "var_48     0.001703\n",
       "var_21     0.001355\n",
       "var_34     0.001336\n",
       "var_91     0.001059\n",
       "var_88     0.000683\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hiển thị top 10 đặc trung gây ra các sự sụt giảm chính\n",
    "# trong roc-auc (hay chất lượng mô hình)\n",
    "## Yêu cầu 4:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "feature_importance.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTXaiCU4rcf_"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "[sort_values()](https://pandas.pydata.org/docs/reference/api/pandas.Series.sort_values.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7vW5e5pno5zm",
    "outputId": "a279c52a-8ac3-4905-8d3c-f210e1ea705c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# số lượng ban đầu của các đặc trưng (trong trường hợp này là hàng)\n",
    "feature_importance.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "q9-CQkD6o5zn",
    "outputId": "c2ae1287-091b-45d4-ba47-1ce7fdf937de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# số lượng đặc trưng khiến chất lượng sụt giảm\n",
    "# khi bị xáo trộn\n",
    "## Yêu cầu 5:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "feature_importance[feature_importance>0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYIHXcNSo5zn"
   },
   "source": [
    "Trong tổng số 108 đặc trưng, có 30 đặc trưng khiến chất lượng của các random forest sụt giảm khi hoán vị các giá trị đặc trưng. Điều này tức là chúng ta có thể lựa chọn các đặc trưng đó, loại bỏ các đặc trưng còn lại, và nên giữ lại chất lượng ban đầu của random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vbIMncezo5zn",
    "outputId": "74dab3ea-4829-458c-efed-79165dc0000c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_7', 'var_11', 'var_13', 'var_16', 'var_21', 'var_25', 'var_31',\n",
       "       'var_34', 'var_38', 'var_43', 'var_46', 'var_48', 'var_55', 'var_58',\n",
       "       'var_65', 'var_66', 'var_68', 'var_69', 'var_70', 'var_73', 'var_74',\n",
       "       'var_79', 'var_88', 'var_91', 'var_92', 'var_96', 'var_98', 'var_104',\n",
       "       'var_105', 'var_108'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in ra các đặc trưng quan trọng\n",
    "## Yêu cầu 6:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "feature_importance[feature_importance>0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G9xoBajo5zo"
   },
   "source": [
    "### Lựa chọn đặc trưng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MFg3NtbQo5zo",
    "outputId": "c3fdf75a-4ec7-4bc5-fb36-2a9d92cc9dd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc score:  0.6954703746877449\n",
      "test auc score:  0.6932896839648326\n"
     ]
    }
   ],
   "source": [
    "# Giờ hãy xây một random forest chỉ với các đặc trưng đã chọn\n",
    "\n",
    "# nắm bắt các đặc trưng đã chọn\n",
    "selected_features = feature_importance[feature_importance > 0].index\n",
    "\n",
    "# huấn luyện random forest mới chỉ dùng các đặc trưng đã chọn\n",
    "rf = RandomForestClassifier(n_estimators=50,\n",
    "                            max_depth=2,\n",
    "                            random_state=2909,\n",
    "                            n_jobs=4)\n",
    "\n",
    "rf.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# in ra roc-auc trên các tập huấn luyện và tập kiểm tra\n",
    "print(\n",
    "    'train auc score: ',\n",
    "    roc_auc_score(y_train, (rf.predict_proba(X_train[selected_features]))[:,\n",
    "                                                                          1]))\n",
    "print(\n",
    "    'test auc score: ',\n",
    "    roc_auc_score(y_test, (rf.predict_proba(X_test[selected_features]))[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0al2cRAgo5zo"
   },
   "source": [
    "Như các bạn thấy, random forest với các đặc trưng đã chọn cho thấy chất lượng tương tự (hoặc thậm chí cao hơn một chút) như các random forest đã xây khi dùng toàn bộ đặc trưng. Nó khiến mô hình đơn giản hơn, nhanh hơn và đáng tin hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndKzRBS6pohK"
   },
   "source": [
    "## Phương pháp lai hóa: Loại bỏ đặc trưng đệ quy\n",
    "\n",
    "Phương pháp này gồm các bước sau:\n",
    "\n",
    "1) Xếp hạng các đặc trưng theo mức độ quan trọng lấy từ thuật toán học máy: có thể là độ quan trọng của cây hoặc các hệ số thu được từ mô hình tuyến tính.\n",
    "\n",
    "2) Loại bỏ đặc trưng ít quan trọng nhất và xây dựng thuật toán học máy với các đặc trưng còn lại.\n",
    "\n",
    "3) Tính toán số liệu chất lượng được chọn: roc-auc, mse, rmse, accuracy,...\n",
    "\n",
    "4) Nếu chỉ số giảm nhiều hơn một ngưỡng được thiết lập tùy ý thì đặc trưng đó quan trọng và cần được giữ lại. Nếu không, chúng ta có thể loại đặc trưng đó.\n",
    "\n",
    "5) Lặp lại các bước 2-4 cho đến khi đánh giá hết tất cả các đặc trưng.\n",
    "\n",
    "\n",
    "Phương pháp này được gọi là lai hóa do:\n",
    "\n",
    "- nó lấy độ quan trọng từ thuật toán học máy như các phương pháp nhúng \n",
    "- nó xây dựng một vài mô hình như các phương pháp gói.\n",
    "\n",
    "Phương pháp này nhanh hơn so với các phương pháp gói và thường tốt hơn các phương pháp nhúng. Thực tế, nó hoạt động rất tốt.\n",
    "\n",
    "Cần lưu ý là lượng giảm chất lượng tối thiểu quyết định liệu một đặc trưng có nên giữ lại hay không được thiết lập tùy ý. Lượng giảm càng nhỏ thì càng có nhiều đặc trưng được chọn và ngược lại.\n",
    "\n",
    "Chúng ta sẽ minh họa cách lựa chọn đặc trưng sử dụng phương thức lai hóa trong bài toán phân loại và hồi quy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lWQcii6RplZj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import roc_auc_score, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXhG52RAptv1"
   },
   "source": [
    "## Phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ULj3VYxkplb-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 109)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tập dữ liệu\n",
    "data = pd.read_csv('dataset_2.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rN-0jkd9ple5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532710</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.349910</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.821374</td>\n",
       "      <td>12.098722</td>\n",
       "      <td>13.309151</td>\n",
       "      <td>4.125599</td>\n",
       "      <td>1.045386</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>8.652883</td>\n",
       "      <td>0.102757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479789</td>\n",
       "      <td>7.795290</td>\n",
       "      <td>3.557890</td>\n",
       "      <td>17.383378</td>\n",
       "      <td>15.193423</td>\n",
       "      <td>8.263673</td>\n",
       "      <td>1.878108</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>1.018818</td>\n",
       "      <td>1.416433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.938776</td>\n",
       "      <td>7.952752</td>\n",
       "      <td>0.972671</td>\n",
       "      <td>3.459267</td>\n",
       "      <td>1.935782</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>2.338139</td>\n",
       "      <td>0.344948</td>\n",
       "      <td>9.937850</td>\n",
       "      <td>11.691283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861487</td>\n",
       "      <td>6.130886</td>\n",
       "      <td>3.401064</td>\n",
       "      <td>15.850471</td>\n",
       "      <td>14.620599</td>\n",
       "      <td>6.849776</td>\n",
       "      <td>1.098210</td>\n",
       "      <td>1.959183</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>1.857893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.020690</td>\n",
       "      <td>9.900544</td>\n",
       "      <td>17.869637</td>\n",
       "      <td>4.366715</td>\n",
       "      <td>1.973693</td>\n",
       "      <td>2.026012</td>\n",
       "      <td>2.853025</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>11.816859</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340944</td>\n",
       "      <td>7.240058</td>\n",
       "      <td>2.417235</td>\n",
       "      <td>15.194609</td>\n",
       "      <td>13.553772</td>\n",
       "      <td>7.229971</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>2.234482</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>2.700606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.909506</td>\n",
       "      <td>10.576516</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>3.419572</td>\n",
       "      <td>1.871438</td>\n",
       "      <td>3.340811</td>\n",
       "      <td>1.868282</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>13.585620</td>\n",
       "      <td>1.153366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.738095</td>\n",
       "      <td>6.565509</td>\n",
       "      <td>4.341414</td>\n",
       "      <td>15.893832</td>\n",
       "      <td>11.929787</td>\n",
       "      <td>6.954033</td>\n",
       "      <td>1.853364</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>2.599562</td>\n",
       "      <td>0.811364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_1      var_2      var_3     var_4     var_5     var_6     var_7  \\\n",
       "0  4.532710   3.280834  17.982476  4.404259  2.349910  0.603264  2.784655   \n",
       "1  5.821374  12.098722  13.309151  4.125599  1.045386  1.832035  1.833494   \n",
       "2  1.938776   7.952752   0.972671  3.459267  1.935782  0.621463  2.338139   \n",
       "3  6.020690   9.900544  17.869637  4.366715  1.973693  2.026012  2.853025   \n",
       "4  3.909506  10.576516   0.934191  3.419572  1.871438  3.340811  1.868282   \n",
       "\n",
       "      var_8      var_9     var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691   0.139346  ...  2.079066  6.748819  2.941445   \n",
       "1  0.709090   8.652883   0.102757  ...  2.479789  7.795290  3.557890   \n",
       "2  0.344948   9.937850  11.691283  ...  1.861487  6.130886  3.401064   \n",
       "3  0.674847  11.816859   0.011151  ...  1.340944  7.240058  2.417235   \n",
       "4  0.439865  13.585620   1.153366  ...  2.738095  6.565509  4.341414   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "1  17.383378  15.193423  8.263673  1.878108  0.567939  1.018818  1.416433  \n",
       "2  15.850471  14.620599  6.849776  1.098210  1.959183  1.575493  1.857893  \n",
       "3  15.194609  13.553772  7.229971  0.835158  2.234482  0.946170  2.700606  \n",
       "4  15.893832  11.929787  6.954033  1.853364  0.511027  2.599562  0.811364  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JtVB0U7pxGX"
   },
   "source": [
    "**Quan trọng**\n",
    "\n",
    "Trong tất cả các quy trình lựa chọn đặc trưng, chỉ nên chọn các đặc trưng bằng cách kiểm tra tập huấn luyện, điều này giúp tránh overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RBookX9kpyeM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tách thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZsRdepwp15-"
   },
   "source": [
    "### Loại các đặc trưng không đổi và gần như không đổi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zXwiBzH_pz_v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\3009294173.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# để tăng tốc độ, hãy loại các đặc trưng không đổi, gần như không đổi và trùng lặp\n",
    "\n",
    "quasi_constant_feat = []\n",
    "\n",
    "# lặp qua từng đặc trưng\n",
    "for feature in X_train.columns:\n",
    "\n",
    "    # tìm các giá trị nổi bật, là các giá trị\n",
    "    # có ở hầu hết các quan sát\n",
    "    predominant = (X_train[feature].value_counts() / np.float(\n",
    "        len(X_train))).sort_values(ascending=False).values[0]\n",
    "\n",
    "    # đánh giá đặc trưng nổi bật: có phải hơn 99% các quan sát\n",
    "    # hiển thị 1 giá trị?\n",
    "    if predominant > 0.998:\n",
    "        \n",
    "        # nếu đúng, hãy thêm biến vào danh sách\n",
    "        quasi_constant_feat.append(feature)\n",
    "\n",
    "X_train.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKAm7ZFWp5z6"
   },
   "source": [
    "### Loại bỏ các đặc trưng trùng lặp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "o2VlI_jlp6XU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_feat = []\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    if i % 10 == 0:  # điều này giúp chúng ta hiểu vòng lặp diễn ra như thế nào\n",
    "        print(i)\n",
    "\n",
    "    col_1 = X_train.columns[i]\n",
    "\n",
    "    for col_2 in X_train.columns[i + 1:]:\n",
    "        if X_train[col_1].equals(X_train[col_2]):\n",
    "            duplicated_feat.append(col_2)\n",
    "            \n",
    "len(duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-0TNkdOop6bH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loại các đặc trưng trùng lặp\n",
    "## Yêu cầu 12:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8Mll_q7sm88"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "Sử dụng ```duplicated_feat```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuM08udmp-oJ"
   },
   "source": [
    "### Xây dựng mô hình học máy với toàn bộ các đặc trưng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "n1Ct6X3vp6dV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC=0.704907\n"
     ]
    }
   ],
   "source": [
    "# bước đầu tiên của quy trình này là xây dựng\n",
    "# thuật toán học máy sử dụng tất cả các đặc trưng hiện có\n",
    "# sau đó xác định độ quan trọng của các đặc trưng\n",
    "# theo thuật toán\n",
    "\n",
    "# xây dựng mô hình ban đầu sử dụng tất cả các đặc trưng\n",
    "model_full = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "model_full.fit(X_train, y_train)\n",
    "\n",
    "# tính roc-auc trong tập kiểm tra\n",
    "y_pred_test = model_full.predict_proba(X_test)[:, 1]\n",
    "roc_full = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print('Test ROC AUC=%f' % (roc_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqIG-SFpqBPI"
   },
   "source": [
    "### Xếp hạng đặc trưng theo độ quan trọng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dSxk5GHtp6fx"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bước thứ hai là độ quan trọng của\n",
    "# từng đặc trưng và xếp theo thứ tự từ ít quan trọng nhất tới\n",
    "# quan trọng nhất\n",
    "\n",
    "# lấy được tên đặc trưng và độ quan trọng\n",
    "## Yêu cầu 13:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "features = pd.Series(model_full.feature_importances_)\n",
    "features.index = X_train.columns\n",
    "\n",
    "# sắp xếp các đặc trưng theo độ quan trọng\n",
    "features.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# vẽ biểu đồ\n",
    "features.plot.bar(figsize=(20,6))\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if9m1C2ws1h7"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "Sử dụng ```feature_importances_```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Rg19Fb3Up6jE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var_1',\n",
       " 'var_74',\n",
       " 'var_73',\n",
       " 'var_72',\n",
       " 'var_71',\n",
       " 'var_68',\n",
       " 'var_65',\n",
       " 'var_64',\n",
       " 'var_63',\n",
       " 'var_76',\n",
       " 'var_62',\n",
       " 'var_59',\n",
       " 'var_58',\n",
       " 'var_57',\n",
       " 'var_56',\n",
       " 'var_52',\n",
       " 'var_51',\n",
       " 'var_50',\n",
       " 'var_49',\n",
       " 'var_60',\n",
       " 'var_77',\n",
       " 'var_78',\n",
       " 'var_79',\n",
       " 'var_107',\n",
       " 'var_106',\n",
       " 'var_105',\n",
       " 'var_103',\n",
       " 'var_102',\n",
       " 'var_101',\n",
       " 'var_100',\n",
       " 'var_98',\n",
       " 'var_97',\n",
       " 'var_95',\n",
       " 'var_93',\n",
       " 'var_92',\n",
       " 'var_90',\n",
       " 'var_89',\n",
       " 'var_87',\n",
       " 'var_83',\n",
       " 'var_82',\n",
       " 'var_81',\n",
       " 'var_80',\n",
       " 'var_46',\n",
       " 'var_45',\n",
       " 'var_54',\n",
       " 'var_43',\n",
       " 'var_26',\n",
       " 'var_25',\n",
       " 'var_24',\n",
       " 'var_23',\n",
       " 'var_44',\n",
       " 'var_20',\n",
       " 'var_19',\n",
       " 'var_13',\n",
       " 'var_11',\n",
       " 'var_10',\n",
       " 'var_8',\n",
       " 'var_7',\n",
       " 'var_6',\n",
       " 'var_5',\n",
       " 'var_4',\n",
       " 'var_3',\n",
       " 'var_2',\n",
       " 'var_27',\n",
       " 'var_29',\n",
       " 'var_109',\n",
       " 'var_33',\n",
       " 'var_37',\n",
       " 'var_35',\n",
       " 'var_39',\n",
       " 'var_40',\n",
       " 'var_38',\n",
       " 'var_41',\n",
       " 'var_42',\n",
       " 'var_31',\n",
       " 'var_47',\n",
       " 'var_18',\n",
       " 'var_12',\n",
       " 'var_66',\n",
       " 'var_86',\n",
       " 'var_85',\n",
       " 'var_17',\n",
       " 'var_28',\n",
       " 'var_15',\n",
       " 'var_99',\n",
       " 'var_53',\n",
       " 'var_22',\n",
       " 'var_14',\n",
       " 'var_104',\n",
       " 'var_9',\n",
       " 'var_67',\n",
       " 'var_108',\n",
       " 'var_84',\n",
       " 'var_32',\n",
       " 'var_96',\n",
       " 'var_75',\n",
       " 'var_34',\n",
       " 'var_30',\n",
       " 'var_36',\n",
       " 'var_94',\n",
       " 'var_48',\n",
       " 'var_70',\n",
       " 'var_21',\n",
       " 'var_91',\n",
       " 'var_69',\n",
       " 'var_88',\n",
       " 'var_16',\n",
       " 'var_55']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tạo danh sách các đặc trưng đã sắp xếp\n",
    "features = list(features.index)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GQUvm5wqFpi"
   },
   "source": [
    "### Lựa chọn đặc trưng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "d91z2I6lqF_q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing recursive feature elimination\n",
      "\n",
      "testing feature:  var_1 1  out of  108\n",
      "New Test ROC AUC=0.7048922495135787\n",
      "Full dataset ROC AUC=0.7048922495135787\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_1\n",
      "\n",
      "testing feature:  var_74 2  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7048922495135787\n",
      "Drop in ROC AUC=-2.1182423100785108e-05\n",
      "remove:  var_74\n",
      "\n",
      "testing feature:  var_73 3  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-2.0475122847862792e-05\n",
      "remove:  var_73\n",
      "\n",
      "testing feature:  var_72 4  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_72\n",
      "\n",
      "testing feature:  var_71 5  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-2.684082512682817e-05\n",
      "remove:  var_71\n",
      "\n",
      "testing feature:  var_68 6  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_68\n",
      "\n",
      "testing feature:  var_65 7  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-6.3657022789653794e-06\n",
      "remove:  var_65\n",
      "\n",
      "testing feature:  var_64 8  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-2.684082512682817e-05\n",
      "remove:  var_64\n",
      "\n",
      "testing feature:  var_63 9  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_63\n",
      "\n",
      "testing feature:  var_76 10  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_76\n",
      "\n",
      "testing feature:  var_62 11  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-2.0475122847862792e-05\n",
      "remove:  var_62\n",
      "\n",
      "testing feature:  var_59 12  out of  108\n",
      "New Test ROC AUC=0.7048858838112995\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=4.8023248227835325e-05\n",
      "remove:  var_59\n",
      "\n",
      "testing feature:  var_58 13  out of  108\n",
      "New Test ROC AUC=0.7048590429861729\n",
      "Full dataset ROC AUC=0.7048858838112995\n",
      "Drop in ROC AUC=2.6840825126606127e-05\n",
      "remove:  var_58\n",
      "\n",
      "testing feature:  var_57 14  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7048590429861729\n",
      "Drop in ROC AUC=-8.122977563340683e-05\n",
      "remove:  var_57\n",
      "\n",
      "testing feature:  var_56 15  out of  108\n",
      "New Test ROC AUC=0.7048590429861729\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=8.122977563340683e-05\n",
      "remove:  var_56\n",
      "\n",
      "testing feature:  var_52 16  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7048590429861729\n",
      "Drop in ROC AUC=-5.438895050657866e-05\n",
      "remove:  var_52\n",
      "\n",
      "testing feature:  var_51 17  out of  108\n",
      "New Test ROC AUC=0.7048654086884518\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=4.802324822761328e-05\n",
      "remove:  var_51\n",
      "\n",
      "testing feature:  var_50 18  out of  108\n",
      "New Test ROC AUC=0.7048654086884518\n",
      "Full dataset ROC AUC=0.7048654086884518\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_50\n",
      "\n",
      "testing feature:  var_49 19  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7048654086884518\n",
      "Drop in ROC AUC=-4.802324822761328e-05\n",
      "remove:  var_49\n",
      "\n",
      "testing feature:  var_60 20  out of  108\n",
      "New Test ROC AUC=0.7048590429861729\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=5.438895050657866e-05\n",
      "remove:  var_60\n",
      "\n",
      "testing feature:  var_77 21  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7048590429861729\n",
      "Drop in ROC AUC=-7.486407335444145e-05\n",
      "remove:  var_77\n",
      "\n",
      "testing feature:  var_78 22  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_78\n",
      "\n",
      "testing feature:  var_79 23  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=-6.3657022789653794e-06\n",
      "remove:  var_79\n",
      "\n",
      "testing feature:  var_107 24  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_107\n",
      "\n",
      "testing feature:  var_106 25  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=-6.3657022789653794e-06\n",
      "remove:  var_106\n",
      "\n",
      "testing feature:  var_105 26  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_105\n",
      "\n",
      "testing feature:  var_103 27  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_103\n",
      "\n",
      "testing feature:  var_102 28  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-3.320652740579355e-05\n",
      "remove:  var_102\n",
      "\n",
      "testing feature:  var_101 29  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_101\n",
      "\n",
      "testing feature:  var_100 30  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=-6.3657022789653794e-06\n",
      "remove:  var_100\n",
      "\n",
      "testing feature:  var_98 31  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_98\n",
      "\n",
      "testing feature:  var_97 32  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_97\n",
      "\n",
      "testing feature:  var_95 33  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-2.684082512682817e-05\n",
      "remove:  var_95\n",
      "\n",
      "testing feature:  var_93 34  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=3.320652740579355e-05\n",
      "remove:  var_93\n",
      "\n",
      "testing feature:  var_92 35  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_92\n",
      "\n",
      "testing feature:  var_90 36  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_90\n",
      "\n",
      "testing feature:  var_89 37  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-3.320652740579355e-05\n",
      "remove:  var_89\n",
      "\n",
      "testing feature:  var_87 38  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_87\n",
      "\n",
      "testing feature:  var_83 39  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_83\n",
      "\n",
      "testing feature:  var_82 40  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=-6.3657022789653794e-06\n",
      "remove:  var_82\n",
      "\n",
      "testing feature:  var_81 41  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_81\n",
      "\n",
      "testing feature:  var_80 42  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_80\n",
      "\n",
      "testing feature:  var_46 43  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=2.0475122847862792e-05\n",
      "remove:  var_46\n",
      "\n",
      "testing feature:  var_45 44  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_45\n",
      "\n",
      "testing feature:  var_54 45  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-6.3657022789653794e-06\n",
      "remove:  var_54\n",
      "\n",
      "testing feature:  var_43 46  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_43\n",
      "\n",
      "testing feature:  var_26 47  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-2.684082512682817e-05\n",
      "remove:  var_26\n",
      "\n",
      "testing feature:  var_25 48  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_25\n",
      "\n",
      "testing feature:  var_24 49  out of  108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=2.0475122847862792e-05\n",
      "remove:  var_24\n",
      "\n",
      "testing feature:  var_23 50  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-2.684082512682817e-05\n",
      "remove:  var_23\n",
      "\n",
      "testing feature:  var_44 51  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_44\n",
      "\n",
      "testing feature:  var_20 52  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_20\n",
      "\n",
      "testing feature:  var_19 53  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_19\n",
      "\n",
      "testing feature:  var_13 54  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-2.684082512682817e-05\n",
      "remove:  var_13\n",
      "\n",
      "testing feature:  var_11 55  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=2.0475122847862792e-05\n",
      "remove:  var_11\n",
      "\n",
      "testing feature:  var_10 56  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-2.684082512682817e-05\n",
      "remove:  var_10\n",
      "\n",
      "testing feature:  var_8 57  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_8\n",
      "\n",
      "testing feature:  var_7 58  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_7\n",
      "\n",
      "testing feature:  var_6 59  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_6\n",
      "\n",
      "testing feature:  var_5 60  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-3.320652740579355e-05\n",
      "remove:  var_5\n",
      "\n",
      "testing feature:  var_4 61  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_4\n",
      "\n",
      "testing feature:  var_3 62  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_3\n",
      "\n",
      "testing feature:  var_2 63  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-6.3657022789653794e-06\n",
      "remove:  var_2\n",
      "\n",
      "testing feature:  var_27 64  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_27\n",
      "\n",
      "testing feature:  var_29 65  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-3.320652740579355e-05\n",
      "remove:  var_29\n",
      "\n",
      "testing feature:  var_109 66  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_109\n",
      "\n",
      "testing feature:  var_33 67  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_33\n",
      "\n",
      "testing feature:  var_37 68  out of  108\n",
      "New Test ROC AUC=0.7049070662344005\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=6.3657022789653794e-06\n",
      "remove:  var_37\n",
      "\n",
      "testing feature:  var_35 69  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049070662344005\n",
      "Drop in ROC AUC=-6.3657022789653794e-06\n",
      "remove:  var_35\n",
      "\n",
      "testing feature:  var_39 70  out of  108\n",
      "New Test ROC AUC=0.7049339070595273\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-2.0475122847862792e-05\n",
      "remove:  var_39\n",
      "\n",
      "testing feature:  var_40 71  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049339070595273\n",
      "Drop in ROC AUC=2.0475122847862792e-05\n",
      "remove:  var_40\n",
      "\n",
      "testing feature:  var_38 72  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-2.684082512682817e-05\n",
      "remove:  var_38\n",
      "\n",
      "testing feature:  var_41 73  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_41\n",
      "\n",
      "testing feature:  var_42 74  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-2.684082512682817e-05\n",
      "remove:  var_42\n",
      "\n",
      "testing feature:  var_31 75  out of  108\n",
      "New Test ROC AUC=0.7049402727618063\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_31\n",
      "\n",
      "testing feature:  var_47 76  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049402727618063\n",
      "Drop in ROC AUC=2.684082512682817e-05\n",
      "remove:  var_47\n",
      "\n",
      "testing feature:  var_18 77  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_18\n",
      "\n",
      "testing feature:  var_12 78  out of  108\n",
      "New Test ROC AUC=0.7049134319366794\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_12\n",
      "\n",
      "testing feature:  var_66 79  out of  108\n",
      "New Test ROC AUC=0.7049205049392119\n",
      "Full dataset ROC AUC=0.7049134319366794\n",
      "Drop in ROC AUC=-7.073002532442807e-06\n",
      "remove:  var_66\n",
      "\n",
      "testing feature:  var_86 80  out of  108\n",
      "New Test ROC AUC=0.7048998468938849\n",
      "Full dataset ROC AUC=0.7049205049392119\n",
      "Drop in ROC AUC=2.0658045327026642e-05\n",
      "remove:  var_86\n",
      "\n",
      "testing feature:  var_85 81  out of  108\n",
      "New Test ROC AUC=0.7049282364626696\n",
      "Full dataset ROC AUC=0.7048998468938849\n",
      "Drop in ROC AUC=-2.8389568784703556e-05\n",
      "remove:  var_85\n",
      "\n",
      "testing feature:  var_17 82  out of  108\n",
      "New Test ROC AUC=0.704837067898995\n",
      "Full dataset ROC AUC=0.7049282364626696\n",
      "Drop in ROC AUC=9.11685636745263e-05\n",
      "remove:  var_17\n",
      "\n",
      "testing feature:  var_28 83  out of  108\n",
      "New Test ROC AUC=0.7043958222944698\n",
      "Full dataset ROC AUC=0.704837067898995\n",
      "Drop in ROC AUC=0.0004412456045251867\n",
      "remove:  var_28\n",
      "\n",
      "testing feature:  var_15 84  out of  108\n",
      "New Test ROC AUC=0.7043440796224968\n",
      "Full dataset ROC AUC=0.7043958222944698\n",
      "Drop in ROC AUC=5.1742671973054044e-05\n",
      "remove:  var_15\n",
      "\n",
      "testing feature:  var_99 85  out of  108\n",
      "New Test ROC AUC=0.7046918030607808\n",
      "Full dataset ROC AUC=0.7043440796224968\n",
      "Drop in ROC AUC=-0.0003477234382840422\n",
      "remove:  var_99\n",
      "\n",
      "testing feature:  var_53 86  out of  108\n",
      "New Test ROC AUC=0.7045163560134851\n",
      "Full dataset ROC AUC=0.7046918030607808\n",
      "Drop in ROC AUC=0.00017544704729577365\n",
      "remove:  var_53\n",
      "\n",
      "testing feature:  var_22 87  out of  108\n",
      "New Test ROC AUC=0.7043494209588917\n",
      "Full dataset ROC AUC=0.7045163560134851\n",
      "Drop in ROC AUC=0.00016693505459330904\n",
      "remove:  var_22\n",
      "\n",
      "testing feature:  var_14 88  out of  108\n",
      "New Test ROC AUC=0.7043597865660511\n",
      "Full dataset ROC AUC=0.7043494209588917\n",
      "Drop in ROC AUC=-1.03656071593905e-05\n",
      "remove:  var_14\n",
      "\n",
      "testing feature:  var_104 89  out of  108\n",
      "New Test ROC AUC=0.704247130708477\n",
      "Full dataset ROC AUC=0.7043597865660511\n",
      "Drop in ROC AUC=0.00011265585757413987\n",
      "remove:  var_104\n",
      "\n",
      "testing feature:  var_9 90  out of  108\n",
      "New Test ROC AUC=0.7042578255760992\n",
      "Full dataset ROC AUC=0.704247130708477\n",
      "Drop in ROC AUC=-1.0694867622174087e-05\n",
      "remove:  var_9\n",
      "\n",
      "testing feature:  var_67 91  out of  108\n",
      "New Test ROC AUC=0.7042144119743493\n",
      "Full dataset ROC AUC=0.7042578255760992\n",
      "Drop in ROC AUC=4.3413601749864306e-05\n",
      "remove:  var_67\n",
      "\n",
      "testing feature:  var_108 92  out of  108\n",
      "New Test ROC AUC=0.7042873980435831\n",
      "Full dataset ROC AUC=0.7042144119743493\n",
      "Drop in ROC AUC=-7.298606923378248e-05\n",
      "remove:  var_108\n",
      "\n",
      "testing feature:  var_84 93  out of  108\n",
      "New Test ROC AUC=0.7042873980435831\n",
      "Full dataset ROC AUC=0.7042873980435831\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  var_84\n",
      "\n",
      "testing feature:  var_32 94  out of  108\n",
      "New Test ROC AUC=0.7041538524388748\n",
      "Full dataset ROC AUC=0.7042873980435831\n",
      "Drop in ROC AUC=0.00013354560470824062\n",
      "remove:  var_32\n",
      "\n",
      "testing feature:  var_96 95  out of  108\n",
      "New Test ROC AUC=0.7040088924714595\n",
      "Full dataset ROC AUC=0.7041538524388748\n",
      "Drop in ROC AUC=0.0001449599674153701\n",
      "remove:  var_96\n",
      "\n",
      "testing feature:  var_75 96  out of  108\n",
      "New Test ROC AUC=0.7042715691517093\n",
      "Full dataset ROC AUC=0.7040088924714595\n",
      "Drop in ROC AUC=-0.00026267668024981816\n",
      "remove:  var_75\n",
      "\n",
      "testing feature:  var_34 97  out of  108\n",
      "New Test ROC AUC=0.7044380408026882\n",
      "Full dataset ROC AUC=0.7042715691517093\n",
      "Drop in ROC AUC=-0.0001664716509789388\n",
      "remove:  var_34\n",
      "\n",
      "testing feature:  var_30 98  out of  108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Test ROC AUC=0.7045188803436991\n",
      "Full dataset ROC AUC=0.7044380408026882\n",
      "Drop in ROC AUC=-8.083954101090196e-05\n",
      "remove:  var_30\n",
      "\n",
      "testing feature:  var_36 99  out of  108\n",
      "New Test ROC AUC=0.7044451260000525\n",
      "Full dataset ROC AUC=0.7045188803436991\n",
      "Drop in ROC AUC=7.375434364664812e-05\n",
      "remove:  var_36\n",
      "\n",
      "testing feature:  var_94 100  out of  108\n",
      "New Test ROC AUC=0.704339945574465\n",
      "Full dataset ROC AUC=0.7044451260000525\n",
      "Drop in ROC AUC=0.00010518042558749219\n",
      "remove:  var_94\n",
      "\n",
      "testing feature:  var_48 101  out of  108\n",
      "New Test ROC AUC=0.7042740081180998\n",
      "Full dataset ROC AUC=0.704339945574465\n",
      "Drop in ROC AUC=6.593745636518378e-05\n",
      "remove:  var_48\n",
      "\n",
      "testing feature:  var_70 102  out of  108\n",
      "New Test ROC AUC=0.7046015491095029\n",
      "Full dataset ROC AUC=0.7042740081180998\n",
      "Drop in ROC AUC=-0.0003275409914030858\n",
      "remove:  var_70\n",
      "\n",
      "testing feature:  var_21 103  out of  108\n",
      "New Test ROC AUC=0.7042253751282743\n",
      "Full dataset ROC AUC=0.7046015491095029\n",
      "Drop in ROC AUC=0.00037617398122857804\n",
      "remove:  var_21\n",
      "\n",
      "testing feature:  var_91 104  out of  108\n",
      "New Test ROC AUC=0.7021944112304646\n",
      "Full dataset ROC AUC=0.7042253751282743\n",
      "Drop in ROC AUC=0.0020309638978097677\n",
      "keep:  var_91\n",
      "\n",
      "testing feature:  var_69 105  out of  108\n",
      "New Test ROC AUC=0.7036708761194095\n",
      "Full dataset ROC AUC=0.7042253751282743\n",
      "Drop in ROC AUC=0.0005544990088648394\n",
      "keep:  var_69\n",
      "\n",
      "testing feature:  var_88 106  out of  108\n",
      "New Test ROC AUC=0.6995139749725465\n",
      "Full dataset ROC AUC=0.7042253751282743\n",
      "Drop in ROC AUC=0.004711400155727796\n",
      "keep:  var_88\n",
      "\n",
      "testing feature:  var_16 107  out of  108\n",
      "New Test ROC AUC=0.697444646133415\n",
      "Full dataset ROC AUC=0.7042253751282743\n",
      "Drop in ROC AUC=0.006780728994859375\n",
      "keep:  var_16\n",
      "\n",
      "testing feature:  var_55 108  out of  108\n",
      "New Test ROC AUC=0.6258049961007025\n",
      "Full dataset ROC AUC=0.7042253751282743\n",
      "Drop in ROC AUC=0.07842037902757182\n",
      "keep:  var_55\n",
      "DONE!!\n",
      "total features to remove:  103\n",
      "total features to keep:  5\n"
     ]
    }
   ],
   "source": [
    "# bước cuối cùng là loại lần lượt\n",
    "# tất cả các đặc trưng, từ ít quan trọng nhất tới quan trọng nhất\n",
    "# và xây dựng mô hình ở từng lượt.\n",
    "\n",
    "# sau khi đã xây dựng mô hình, tính roc-auc mới\n",
    "\n",
    "# nếu roc-auc mới nhỏ hơn roc-auc ban đầu\n",
    "# (với tất cả đặc trưng) thì đặc trưng bị loại\n",
    "# quan trọng, chúng ta cần giữ nó lại.\n",
    "# nếu không, hãy loại bỏ nó\n",
    "\n",
    "# loại đặc trưng đệ quy:\n",
    "\n",
    "# trước tiên, chúng ta tùy ý thiết lập mức giảm trong roc-auc\n",
    "# nếu mức giảm nằm dưới ngưỡng này,\n",
    "# đặc trưng sẽ bị loại\n",
    "tol = 0.0005\n",
    "\n",
    "print('doing recursive feature elimination')\n",
    "\n",
    "# hãy khởi tạo một danh sách thu thập\n",
    "# các đặc trưng nên loại\n",
    "features_to_remove = []\n",
    "\n",
    "# cài đặt bộ đếm để biết vòng lặp ở đâu\n",
    "count = 1\n",
    "\n",
    "# giờ chúng ta lặp qua toàn bộ các đặc trưng, theo độ quan trọng:\n",
    "# nhớ rằng các đặc trưng trong danh sách này được sắp xếp theo\n",
    "# độ quan trọng\n",
    "for feature in features:\n",
    "    \n",
    "    print()\n",
    "    print('testing feature: ', feature, count, ' out of ', len(features))\n",
    "    count = count + 1\n",
    "\n",
    "    # khởi tạo mô hình\n",
    "    model_int = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "    # khớp mô hình với tất cả các biến, trừ đặc trưng được đánh giá\n",
    "    # cũng như tất cả các các đối tượng được coi là bị loại\n",
    "    \n",
    "    # features_to_remove sẽ rỗng ở các lượt đầu tiên\n",
    "    # nhưng sẽ có các đặc trưng khi tiến hành vòng lặp\n",
    "    model_int.fit(\n",
    "        X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
    "\n",
    "    # đưa ra dự đoán, sử dụng tập kiểm tra\n",
    "    y_pred_test = model_int.predict_proba(\n",
    "        X_test.drop(features_to_remove + [feature], axis=1))[:, 1]\n",
    "\n",
    "    # tính toán roc-auc mới\n",
    "    roc_int = roc_auc_score(y_test, y_pred_test)\n",
    "    print('New Test ROC AUC={}'.format((roc_int)))\n",
    "\n",
    "    # in ra roc-auc ban đầu với tất cả các đặc trưng\n",
    "    print('Full dataset ROC AUC={}'.format((roc_full)))\n",
    "\n",
    "    # xác định lượng giảm trong roc-auc\n",
    "    diff_roc = roc_full - roc_int\n",
    "\n",
    "    # so sánh lượng giảm trong roc-auc với dung sai\n",
    "    # chúng ta đã thiết lập trước đó\n",
    "    if diff_roc >= tol:\n",
    "        print('Drop in ROC AUC={}'.format(diff_roc))\n",
    "        print('keep: ', feature)\n",
    "        print\n",
    "    else:\n",
    "        print('Drop in ROC AUC={}'.format(diff_roc))\n",
    "        print('remove: ', feature)\n",
    "        print\n",
    "        # nếu lượng giảm trong roc-auc nhỏ và chúng ta loại đặc trưng,\n",
    "        # cần đặt roc-auc mới dựa trên \n",
    "        # các đặc trưng còn lại\n",
    "        roc_full = roc_int\n",
    "        \n",
    "        # và thêm các đặc trưng cần loại vào danh sách thu thập\n",
    "        ## Yêu cầu 14:\n",
    "        ## VIẾT CODE Ở ĐÂY:\n",
    "        features_to_remove.append(feature)\n",
    "\n",
    "# vòng lặp giờ đã hoàn thành, hãy đánh giá toàn bộ đặc trưng\n",
    "print('DONE!!')\n",
    "## Yêu cầu 15:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "print('total features to remove: ', len(features_to_remove))\n",
    "\n",
    "# xác định các đặc trưng cần giữ (các đặc trưng mà chúng ta sẽ không loại)\n",
    "features_to_keep = [x for x in features if x not in features_to_remove]\n",
    "print('total features to keep: ', len(features_to_keep))\n",
    "\n",
    "# Hãy chờ một lúc, điều này tốn chút thời gian!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fRuJy4paqGBo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test selected features ROC AUC=0.704225\n"
     ]
    }
   ],
   "source": [
    "#  để so sánh, chúng ta sẽ xây dựng một mô hình chỉ với các đặc trưng đã chọn\n",
    "\n",
    "model_final = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "# khớp mô hình với các đặc trưng đã chọn\n",
    "model_final.fit(X_train[features_to_keep], y_train)\n",
    "\n",
    "# đưa ra dự đoán\n",
    "## Yêu cầu 16:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "y_pred_test = model_final.predict_proba(X_test[features_to_keep])[:, 1]\n",
    "\n",
    "# tính roc-auc\n",
    "roc_final = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test selected features ROC AUC=%f' % (roc_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkoKtHI3toUx"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "Sử dụng ```predict_proba()```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtUoS2CYqMWu"
   },
   "source": [
    "Như các bạn thấy, mô hình Gradient Boosting với 12 đặc trưng cho thấy chất lượng tương tự với mô hình được xây với tập dữ liệu đầy đủ (cuộn lên trên để tìm giá trị này, chúng ta đã tính một vài cell trước đó).\n",
    "\n",
    "**Bài tập:**\n",
    "Hãy thử các giá trị dung sai khác. Thử với các ngưỡng nhỏ hơn hoặc lớn hơn để hiểu được tác động của điều này tới số lượng các đặc trưng được chọn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YETn1nk7qn5v"
   },
   "source": [
    "## Phương pháp lai hóa: Loại bỏ đặc trưng đệ quy\n",
    "\n",
    "Phương pháp này gồm các bước sau:\n",
    "\n",
    "1) Xếp hạng các đặc trưng theo mức độ quan trọng lấy từ thuật toán học máy: có thể là độ quan trọng của cây hoặc các hệ số thu được từ mô hình tuyến tính.\n",
    "\n",
    "2) Loại bỏ đặc trưng ít quan trọng nhất và xây dựng thuật toán học máy với các đặc trưng còn lại.\n",
    "\n",
    "3) Tính toán số liệu chất lượng được chọn: roc-auc, mse, rmse, accuracy,...\n",
    "\n",
    "4) Nếu chỉ số giảm nhiều hơn một ngưỡng được thiết lập tùy ý thì đặc trưng đó quan trọng và cần được giữ lại. Nếu không, chúng ta có thể loại đặc trưng đó.\n",
    "\n",
    "5) Lặp lại các bước 2-4 cho đến khi đánh giá hết tất cả các đặc trưng.\n",
    "\n",
    "\n",
    "Phương pháp này được gọi là lai hóa do:\n",
    "\n",
    "- nó lấy độ quan trọng từ thuật toán học máy như các phương pháp nhúng \n",
    "- nó xây dựng một vài mô hình như các phương pháp gói.\n",
    "\n",
    "Phương pháp này nhanh hơn so với các phương pháp gói và thường tốt hơn các phương pháp nhúng. Thực tế, nó hoạt động rất tốt.\n",
    "\n",
    "Cần lưu ý là lượng giảm chất lượng tối thiểu quyết định liệu một đặc trưng có nên giữ lại hay không được thiết lập tùy ý. Lượng giảm càng nhỏ thì càng có nhiều đặc trưng được chọn và ngược lại.\n",
    "\n",
    "Chúng ta sẽ minh họa cách lựa chọn đặc trưng sử dụng phương thức lai hóa trong bài toán phân loại và hồi quy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dhcIi8FgqpmM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import roc_auc_score, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIOLgMRNqq3m"
   },
   "source": [
    "## Phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Lnio2hDiqVDb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 109)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tập dữ liệu\n",
    "data = pd.read_csv('dataset_2.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "K9z02LSOqsUK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532710</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.349910</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.821374</td>\n",
       "      <td>12.098722</td>\n",
       "      <td>13.309151</td>\n",
       "      <td>4.125599</td>\n",
       "      <td>1.045386</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>8.652883</td>\n",
       "      <td>0.102757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479789</td>\n",
       "      <td>7.795290</td>\n",
       "      <td>3.557890</td>\n",
       "      <td>17.383378</td>\n",
       "      <td>15.193423</td>\n",
       "      <td>8.263673</td>\n",
       "      <td>1.878108</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>1.018818</td>\n",
       "      <td>1.416433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.938776</td>\n",
       "      <td>7.952752</td>\n",
       "      <td>0.972671</td>\n",
       "      <td>3.459267</td>\n",
       "      <td>1.935782</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>2.338139</td>\n",
       "      <td>0.344948</td>\n",
       "      <td>9.937850</td>\n",
       "      <td>11.691283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861487</td>\n",
       "      <td>6.130886</td>\n",
       "      <td>3.401064</td>\n",
       "      <td>15.850471</td>\n",
       "      <td>14.620599</td>\n",
       "      <td>6.849776</td>\n",
       "      <td>1.098210</td>\n",
       "      <td>1.959183</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>1.857893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.020690</td>\n",
       "      <td>9.900544</td>\n",
       "      <td>17.869637</td>\n",
       "      <td>4.366715</td>\n",
       "      <td>1.973693</td>\n",
       "      <td>2.026012</td>\n",
       "      <td>2.853025</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>11.816859</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340944</td>\n",
       "      <td>7.240058</td>\n",
       "      <td>2.417235</td>\n",
       "      <td>15.194609</td>\n",
       "      <td>13.553772</td>\n",
       "      <td>7.229971</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>2.234482</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>2.700606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.909506</td>\n",
       "      <td>10.576516</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>3.419572</td>\n",
       "      <td>1.871438</td>\n",
       "      <td>3.340811</td>\n",
       "      <td>1.868282</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>13.585620</td>\n",
       "      <td>1.153366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.738095</td>\n",
       "      <td>6.565509</td>\n",
       "      <td>4.341414</td>\n",
       "      <td>15.893832</td>\n",
       "      <td>11.929787</td>\n",
       "      <td>6.954033</td>\n",
       "      <td>1.853364</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>2.599562</td>\n",
       "      <td>0.811364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_1      var_2      var_3     var_4     var_5     var_6     var_7  \\\n",
       "0  4.532710   3.280834  17.982476  4.404259  2.349910  0.603264  2.784655   \n",
       "1  5.821374  12.098722  13.309151  4.125599  1.045386  1.832035  1.833494   \n",
       "2  1.938776   7.952752   0.972671  3.459267  1.935782  0.621463  2.338139   \n",
       "3  6.020690   9.900544  17.869637  4.366715  1.973693  2.026012  2.853025   \n",
       "4  3.909506  10.576516   0.934191  3.419572  1.871438  3.340811  1.868282   \n",
       "\n",
       "      var_8      var_9     var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691   0.139346  ...  2.079066  6.748819  2.941445   \n",
       "1  0.709090   8.652883   0.102757  ...  2.479789  7.795290  3.557890   \n",
       "2  0.344948   9.937850  11.691283  ...  1.861487  6.130886  3.401064   \n",
       "3  0.674847  11.816859   0.011151  ...  1.340944  7.240058  2.417235   \n",
       "4  0.439865  13.585620   1.153366  ...  2.738095  6.565509  4.341414   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "1  17.383378  15.193423  8.263673  1.878108  0.567939  1.018818  1.416433  \n",
       "2  15.850471  14.620599  6.849776  1.098210  1.959183  1.575493  1.857893  \n",
       "3  15.194609  13.553772  7.229971  0.835158  2.234482  0.946170  2.700606  \n",
       "4  15.893832  11.929787  6.954033  1.853364  0.511027  2.599562  0.811364  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Yc0kVLnqt42"
   },
   "source": [
    "**Quan trọng**\n",
    "\n",
    "Trong tất cả các quy trình lựa chọn đặc trưng, chỉ nên chọn các đặc trưng bằng cách kiểm tra tập huấn luyện, điều này giúp tránh overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "W2vJNH-wqsWN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tách thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSgv57pVq1e8"
   },
   "source": [
    "### Loại các đặc trưng không đổi và gần như không đổi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "VeGzp19SqsYW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n",
      "C:\\Users\\NCPC\\AppData\\Local\\Temp\\ipykernel_11976\\1175622431.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predominant = (X_train[feature].value_counts() / np.float(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# để tăng tốc độ, hãy loại các đặc trưng không đổi, gần như không đổi và trùng lặp\n",
    "\n",
    "quasi_constant_feat = []\n",
    "\n",
    "# lặp qua từng đặc trưng\n",
    "for feature in X_train.columns:\n",
    "\n",
    "    # tìm các giá trị nổi bật, là các giá trị\n",
    "    # có ở hầu hết các quan sát\n",
    "    predominant = (X_train[feature].value_counts() / np.float(\n",
    "        len(X_train))).sort_values(ascending=False).values[0]\n",
    "\n",
    "    # đánh giá đặc trưng nổi bật: có phải hơn 99% các quan sát\n",
    "    # hiển thị 1 giá trị?\n",
    "    if predominant > 0.998:\n",
    "        \n",
    "        # nếu đúng, hãy thêm biến vào danh sách\n",
    "        quasi_constant_feat.append(feature)\n",
    "\n",
    "## Yêu cầu 20:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "X_train.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx6FmLrevC3n"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "Sử dụng ```quasi_constant_feat```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or1yP4i1q49S"
   },
   "source": [
    "### Loại các đặc trưng trùng lặp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "2GC3nT1pq3iI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_feat = []\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    if i % 10 == 0:  # điều này giúp chúng ta hiểu vòng lặp diễn ra như thế nào\n",
    "        print(i)\n",
    "\n",
    "    col_1 = X_train.columns[i]\n",
    "\n",
    "    for col_2 in X_train.columns[i + 1:]:\n",
    "        ## Yêu cầu 21:\n",
    "        ## VIẾT CODE Ở ĐÂY:\n",
    "        if X_train[col_1].equals(X_train[col_2]):\n",
    "            duplicated_feat.append(...)\n",
    "            \n",
    "len(duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Wt5AXVpwq3ks"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loại các đặc trưng trùng lặp\n",
    "## Yêu cầu 22:\n",
    "## VIẾT CODE Ở ĐÂY:\n",
    "X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRtNiZs8vdDe"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "Sử dụng ```duplicated_feat```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtXU5lX8q8_7"
   },
   "source": [
    "### Huấn luyện mô hình học máy với toàn bộ các đặc trưng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MEKONd21q3nB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC=0.704907\n"
     ]
    }
   ],
   "source": [
    "# bước đầu tiên của quy trình này là xây dựng\n",
    "# thuật toán học máy sử dụng tất cả các đặc trưng hiện có\n",
    "# sau đó xác định độ quan trọng của các đặc trưng\n",
    "# theo thuật toán\n",
    "\n",
    "# xây dựng mô hình ban đầu sử dụng tất cả các đặc trưng\n",
    "model_full = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "model_full.fit(X_train, y_train)\n",
    "\n",
    "# tính roc-auc trong tập kiểm tra\n",
    "y_pred_test = model_full.predict_proba(X_test)[:, 1]\n",
    "roc_full = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print('Test ROC AUC=%f' % (roc_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLDPTYofrEGM"
   },
   "source": [
    "### Xếp hạng đặc trưng theo độ quan trọng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "LFYpHgixqsa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Feature')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bước thứ hai là độ quan trọng của\n",
    "# từng đặc trưng theo thứ tự từ ít quan trọng nhất tới\n",
    "# quan trọng nhất\n",
    "\n",
    "# lấy được tên đặc trưng và độ quan trọng\n",
    "features = pd.Series(model_full.feature_importances_)\n",
    "features.index = X_train.columns\n",
    "\n",
    "# sắp xếp các đặc trưng theo độ quan trọng\n",
    "features.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "# vẽ biểu đồ\n",
    "features.plot.bar(figsize=(20,6))\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "h_lRd5ASrHLN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var_55',\n",
       " 'var_16',\n",
       " 'var_88',\n",
       " 'var_69',\n",
       " 'var_91',\n",
       " 'var_21',\n",
       " 'var_70',\n",
       " 'var_48',\n",
       " 'var_94',\n",
       " 'var_36',\n",
       " 'var_30',\n",
       " 'var_34',\n",
       " 'var_75',\n",
       " 'var_96',\n",
       " 'var_32',\n",
       " 'var_84',\n",
       " 'var_108',\n",
       " 'var_67',\n",
       " 'var_9',\n",
       " 'var_104',\n",
       " 'var_14',\n",
       " 'var_22',\n",
       " 'var_53',\n",
       " 'var_99',\n",
       " 'var_15',\n",
       " 'var_28',\n",
       " 'var_17',\n",
       " 'var_85',\n",
       " 'var_86',\n",
       " 'var_66',\n",
       " 'var_12',\n",
       " 'var_18',\n",
       " 'var_47',\n",
       " 'var_74',\n",
       " 'var_78',\n",
       " 'var_77',\n",
       " 'var_76',\n",
       " 'var_106',\n",
       " 'var_97',\n",
       " 'var_73',\n",
       " 'var_72',\n",
       " 'var_71',\n",
       " 'var_107',\n",
       " 'var_98',\n",
       " 'var_68',\n",
       " 'var_105',\n",
       " 'var_101',\n",
       " 'var_102',\n",
       " 'var_80',\n",
       " 'var_81',\n",
       " 'var_82',\n",
       " 'var_83',\n",
       " 'var_100',\n",
       " 'var_95',\n",
       " 'var_87',\n",
       " 'var_93',\n",
       " 'var_103',\n",
       " 'var_89',\n",
       " 'var_92',\n",
       " 'var_90',\n",
       " 'var_65',\n",
       " 'var_79',\n",
       " 'var_1',\n",
       " 'var_64',\n",
       " 'var_37',\n",
       " 'var_33',\n",
       " 'var_31',\n",
       " 'var_29',\n",
       " 'var_27',\n",
       " 'var_26',\n",
       " 'var_25',\n",
       " 'var_24',\n",
       " 'var_23',\n",
       " 'var_20',\n",
       " 'var_19',\n",
       " 'var_13',\n",
       " 'var_11',\n",
       " 'var_10',\n",
       " 'var_8',\n",
       " 'var_7',\n",
       " 'var_6',\n",
       " 'var_5',\n",
       " 'var_4',\n",
       " 'var_3',\n",
       " 'var_35',\n",
       " 'var_38',\n",
       " 'var_63',\n",
       " 'var_39',\n",
       " 'var_62',\n",
       " 'var_60',\n",
       " 'var_59',\n",
       " 'var_58',\n",
       " 'var_57',\n",
       " 'var_56',\n",
       " 'var_2',\n",
       " 'var_54',\n",
       " 'var_52',\n",
       " 'var_51',\n",
       " 'var_50',\n",
       " 'var_49',\n",
       " 'var_46',\n",
       " 'var_45',\n",
       " 'var_44',\n",
       " 'var_43',\n",
       " 'var_42',\n",
       " 'var_41',\n",
       " 'var_40',\n",
       " 'var_109']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tạo danh sách các đặc trưng đã sắp xếp\n",
    "features = list(features.index)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "culGaVC7rJpw"
   },
   "source": [
    "### Xây dựng mô hình học máy với 1 đặc trưng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "qNu_jAoorKQl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test one feature xgb ROC AUC=0.681283\n"
     ]
    }
   ],
   "source": [
    "# tiếp theo, chúng ta cần xây dựng mô hình học máy \n",
    "# chỉ sử dụng đặc trưng quan trọng nhất\n",
    "\n",
    "# xây dựng mô hình ban đầu với tất cả các đặc trưng\n",
    "model_one_feature = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "# chỉ huấn luyện với đặc trưng quan trọng nhất\n",
    "model_one_feature.fit(X_train[features[0]].to_frame(), y_train)\n",
    "\n",
    "# tính roc-auc trong tập kiểm tra\n",
    "y_pred_test = model_one_feature.predict_proba(X_test[features[0]].to_frame())[:, 1]\n",
    "\n",
    "roc_first = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print('Test one feature xgb ROC AUC=%f' % (roc_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_CYB_wjrNc-"
   },
   "source": [
    "### Lựa chọn đặc trưng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "-cS7wwVwrL9p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing recursive feature addition\n",
      "\n",
      "testing feature:  var_16 1  out of  108\n",
      "New Test ROC AUC=0.6954212674210795\n",
      "Previous round Test ROC AUC=0.681282652424729\n",
      "Increase in ROC AUC=0.014138614996350496\n",
      "keep:  var_16\n",
      "\n",
      "testing feature:  var_88 2  out of  108\n",
      "New Test ROC AUC=0.701334626798509\n",
      "Previous round Test ROC AUC=0.6954212674210795\n",
      "Increase in ROC AUC=0.005913359377429539\n",
      "keep:  var_88\n",
      "\n",
      "testing feature:  var_69 3  out of  108\n",
      "New Test ROC AUC=0.7021944112304646\n",
      "Previous round Test ROC AUC=0.701334626798509\n",
      "Increase in ROC AUC=0.000859784431955557\n",
      "keep:  var_69\n",
      "\n",
      "testing feature:  var_91 4  out of  108\n",
      "New Test ROC AUC=0.7042253751282743\n",
      "Previous round Test ROC AUC=0.7021944112304646\n",
      "Increase in ROC AUC=0.0020309638978097677\n",
      "keep:  var_91\n",
      "\n",
      "testing feature:  var_21 5  out of  108\n",
      "New Test ROC AUC=0.7046015491095029\n",
      "Previous round Test ROC AUC=0.7042253751282743\n",
      "Increase in ROC AUC=0.00037617398122857804\n",
      "keep:  var_21\n",
      "\n",
      "testing feature:  var_70 6  out of  108\n",
      "New Test ROC AUC=0.7042740081180998\n",
      "Previous round Test ROC AUC=0.7046015491095029\n",
      "Increase in ROC AUC=-0.0003275409914030858\n",
      "remove:  var_70\n",
      "\n",
      "testing feature:  var_48 7  out of  108\n",
      "New Test ROC AUC=0.7044757350282523\n",
      "Previous round Test ROC AUC=0.7046015491095029\n",
      "Increase in ROC AUC=-0.0001258140812505637\n",
      "remove:  var_48\n",
      "\n",
      "testing feature:  var_94 8  out of  108\n",
      "New Test ROC AUC=0.7047128879252262\n",
      "Previous round Test ROC AUC=0.7046015491095029\n",
      "Increase in ROC AUC=0.00011133881572333859\n",
      "keep:  var_94\n",
      "\n",
      "testing feature:  var_36 9  out of  108\n",
      "New Test ROC AUC=0.7047064734436193\n",
      "Previous round Test ROC AUC=0.7047128879252262\n",
      "Increase in ROC AUC=-6.414481606986655e-06\n",
      "remove:  var_36\n",
      "\n",
      "testing feature:  var_30 10  out of  108\n",
      "New Test ROC AUC=0.7043688717158555\n",
      "Previous round Test ROC AUC=0.7047128879252262\n",
      "Increase in ROC AUC=-0.0003440162093707455\n",
      "remove:  var_30\n",
      "\n",
      "testing feature:  var_34 11  out of  108\n",
      "New Test ROC AUC=0.7047281558548303\n",
      "Previous round Test ROC AUC=0.7047128879252262\n",
      "Increase in ROC AUC=1.5267929604045882e-05\n",
      "remove:  var_34\n",
      "\n",
      "testing feature:  var_75 12  out of  108\n",
      "New Test ROC AUC=0.7043277385476808\n",
      "Previous round Test ROC AUC=0.7047128879252262\n",
      "Increase in ROC AUC=-0.0003851493775454129\n",
      "remove:  var_75\n",
      "\n",
      "testing feature:  var_96 13  out of  108\n",
      "New Test ROC AUC=0.7049394800977293\n",
      "Previous round Test ROC AUC=0.7047128879252262\n",
      "Increase in ROC AUC=0.00022659217250309283\n",
      "keep:  var_96\n",
      "\n",
      "testing feature:  var_32 14  out of  108\n",
      "New Test ROC AUC=0.7046954249258708\n",
      "Previous round Test ROC AUC=0.7049394800977293\n",
      "Increase in ROC AUC=-0.0002440551718585482\n",
      "remove:  var_32\n",
      "\n",
      "testing feature:  var_84 15  out of  108\n",
      "New Test ROC AUC=0.7048012638723835\n",
      "Previous round Test ROC AUC=0.7049394800977293\n",
      "Increase in ROC AUC=-0.0001382162253458219\n",
      "remove:  var_84\n",
      "\n",
      "testing feature:  var_108 16  out of  108\n",
      "New Test ROC AUC=0.7045281606108147\n",
      "Previous round Test ROC AUC=0.7049394800977293\n",
      "Increase in ROC AUC=-0.0004113194869146408\n",
      "remove:  var_108\n",
      "\n",
      "testing feature:  var_67 17  out of  108\n",
      "New Test ROC AUC=0.7049638331771378\n",
      "Previous round Test ROC AUC=0.7049394800977293\n",
      "Increase in ROC AUC=2.435307940851228e-05\n",
      "remove:  var_67\n",
      "\n",
      "testing feature:  var_9 18  out of  108\n",
      "New Test ROC AUC=0.7048971152515277\n",
      "Previous round Test ROC AUC=0.7049394800977293\n",
      "Increase in ROC AUC=-4.236484620168124e-05\n",
      "remove:  var_9\n",
      "\n",
      "testing feature:  var_104 19  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7049394800977293\n",
      "Increase in ROC AUC=0.000141374686821516\n",
      "keep:  var_104\n",
      "\n",
      "testing feature:  var_14 20  out of  108\n",
      "New Test ROC AUC=0.7047810204513429\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00029983433320790454\n",
      "remove:  var_14\n",
      "\n",
      "testing feature:  var_22 21  out of  108\n",
      "New Test ROC AUC=0.7050719647520577\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-8.89003249315845e-06\n",
      "remove:  var_22\n",
      "\n",
      "testing feature:  var_53 22  out of  108\n",
      "New Test ROC AUC=0.7047991297767919\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00028172500775891507\n",
      "remove:  var_53\n",
      "\n",
      "testing feature:  var_99 23  out of  108\n",
      "New Test ROC AUC=0.7049940641655472\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-8.679061900362939e-05\n",
      "remove:  var_99\n",
      "\n",
      "testing feature:  var_15 24  out of  108\n",
      "New Test ROC AUC=0.7050090150295207\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-7.183975503011197e-05\n",
      "remove:  var_15\n",
      "\n",
      "testing feature:  var_28 25  out of  108\n",
      "New Test ROC AUC=0.7051052200587913\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=2.436527424043433e-05\n",
      "remove:  var_28\n",
      "\n",
      "testing feature:  var_17 26  out of  108\n",
      "New Test ROC AUC=0.7050858058863234\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=4.951101772565636e-06\n",
      "remove:  var_17\n",
      "\n",
      "testing feature:  var_85 27  out of  108\n",
      "New Test ROC AUC=0.7050454775770576\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-3.537720749324791e-05\n",
      "remove:  var_85\n",
      "\n",
      "testing feature:  var_86 28  out of  108\n",
      "New Test ROC AUC=0.7050809035638785\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=4.8779327688208696e-08\n",
      "remove:  var_86\n",
      "\n",
      "testing feature:  var_66 29  out of  108\n",
      "New Test ROC AUC=0.7050668307278058\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-1.4024056744998958e-05\n",
      "remove:  var_66\n",
      "\n",
      "testing feature:  var_12 30  out of  108\n",
      "New Test ROC AUC=0.7051034518081583\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=2.2597023607406896e-05\n",
      "remove:  var_12\n",
      "\n",
      "testing feature:  var_18 31  out of  108\n",
      "New Test ROC AUC=0.7050945008015053\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=1.364601695441614e-05\n",
      "remove:  var_18\n",
      "\n",
      "testing feature:  var_47 32  out of  108\n",
      "New Test ROC AUC=0.7049524066195987\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00012844816495216627\n",
      "remove:  var_47\n",
      "\n",
      "testing feature:  var_74 33  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_74\n",
      "\n",
      "testing feature:  var_78 34  out of  108\n",
      "New Test ROC AUC=0.7048630306962212\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00021782408832959899\n",
      "remove:  var_78\n",
      "\n",
      "testing feature:  var_77 35  out of  108\n",
      "New Test ROC AUC=0.7049526871007336\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00012816768381729293\n",
      "remove:  var_77\n",
      "\n",
      "testing feature:  var_76 36  out of  108\n",
      "New Test ROC AUC=0.7050355143993527\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-4.5340385198100464e-05\n",
      "remove:  var_76\n",
      "\n",
      "testing feature:  var_106 37  out of  108\n",
      "New Test ROC AUC=0.7050103198765395\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-7.053490801134377e-05\n",
      "remove:  var_106\n",
      "\n",
      "testing feature:  var_97 38  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_97\n",
      "\n",
      "testing feature:  var_73 39  out of  108\n",
      "New Test ROC AUC=0.7050086491845622\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-7.220559998866172e-05\n",
      "remove:  var_73\n",
      "\n",
      "testing feature:  var_72 40  out of  108\n",
      "New Test ROC AUC=0.7049452360584108\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.0001356187261400965\n",
      "remove:  var_72\n",
      "\n",
      "testing feature:  var_71 41  out of  108\n",
      "New Test ROC AUC=0.7049765279972001\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00010432678735072809\n",
      "remove:  var_71\n",
      "\n",
      "testing feature:  var_107 42  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_107\n",
      "\n",
      "testing feature:  var_98 43  out of  108\n",
      "New Test ROC AUC=0.7050930740061669\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=1.2219221616094345e-05\n",
      "remove:  var_98\n",
      "\n",
      "testing feature:  var_68 44  out of  108\n",
      "New Test ROC AUC=0.7051158539522536\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=3.499916770277611e-05\n",
      "remove:  var_68\n",
      "\n",
      "testing feature:  var_105 45  out of  108\n",
      "New Test ROC AUC=0.7049675526008833\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00011330218366756295\n",
      "remove:  var_105\n",
      "\n",
      "testing feature:  var_101 46  out of  108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Test ROC AUC=0.705054318830223\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-2.653595432788869e-05\n",
      "remove:  var_101\n",
      "\n",
      "testing feature:  var_102 47  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_102\n",
      "\n",
      "testing feature:  var_80 48  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_80\n",
      "\n",
      "testing feature:  var_81 49  out of  108\n",
      "New Test ROC AUC=0.7049760889832498\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00010476580130103219\n",
      "remove:  var_81\n",
      "\n",
      "testing feature:  var_82 50  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_82\n",
      "\n",
      "testing feature:  var_83 51  out of  108\n",
      "New Test ROC AUC=0.7051380119619106\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=5.7157177359767886e-05\n",
      "remove:  var_83\n",
      "\n",
      "testing feature:  var_100 52  out of  108\n",
      "New Test ROC AUC=0.7050930740061669\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=1.2219221616094345e-05\n",
      "remove:  var_100\n",
      "\n",
      "testing feature:  var_95 53  out of  108\n",
      "New Test ROC AUC=0.7050378801967514\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-4.297458779944918e-05\n",
      "remove:  var_95\n",
      "\n",
      "testing feature:  var_87 54  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_87\n",
      "\n",
      "testing feature:  var_93 55  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_93\n",
      "\n",
      "testing feature:  var_103 56  out of  108\n",
      "New Test ROC AUC=0.7050997445792448\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=1.8889794693999207e-05\n",
      "remove:  var_103\n",
      "\n",
      "testing feature:  var_89 57  out of  108\n",
      "New Test ROC AUC=0.7051034518081583\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=2.2597023607406896e-05\n",
      "remove:  var_89\n",
      "\n",
      "testing feature:  var_92 58  out of  108\n",
      "New Test ROC AUC=0.7051280731738697\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=4.721838931887046e-05\n",
      "remove:  var_92\n",
      "\n",
      "testing feature:  var_90 59  out of  108\n",
      "New Test ROC AUC=0.7050015152078701\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-7.933957668071479e-05\n",
      "remove:  var_90\n",
      "\n",
      "testing feature:  var_65 60  out of  108\n",
      "New Test ROC AUC=0.7051034518081583\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=2.2597023607406896e-05\n",
      "remove:  var_65\n",
      "\n",
      "testing feature:  var_79 61  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_79\n",
      "\n",
      "testing feature:  var_1 62  out of  108\n",
      "New Test ROC AUC=0.7050912935607019\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=1.0438776151033835e-05\n",
      "remove:  var_1\n",
      "\n",
      "testing feature:  var_64 63  out of  108\n",
      "New Test ROC AUC=0.7051153661589755\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=3.451137442467278e-05\n",
      "remove:  var_64\n",
      "\n",
      "testing feature:  var_37 64  out of  108\n",
      "New Test ROC AUC=0.7050339412660309\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-4.6913518519930975e-05\n",
      "remove:  var_37\n",
      "\n",
      "testing feature:  var_33 65  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_33\n",
      "\n",
      "testing feature:  var_31 66  out of  108\n",
      "New Test ROC AUC=0.7051434142724656\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=6.255948791478172e-05\n",
      "remove:  var_31\n",
      "\n",
      "testing feature:  var_29 67  out of  108\n",
      "New Test ROC AUC=0.7050946837239846\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=1.3828939433802034e-05\n",
      "remove:  var_29\n",
      "\n",
      "testing feature:  var_27 68  out of  108\n",
      "New Test ROC AUC=0.7051589139038767\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=7.80591193259017e-05\n",
      "remove:  var_27\n",
      "\n",
      "testing feature:  var_26 69  out of  108\n",
      "New Test ROC AUC=0.705051757915513\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-2.9096869037847917e-05\n",
      "remove:  var_26\n",
      "\n",
      "testing feature:  var_25 70  out of  108\n",
      "New Test ROC AUC=0.7050064297251467\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-7.442505940413735e-05\n",
      "remove:  var_25\n",
      "\n",
      "testing feature:  var_24 71  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_24\n",
      "\n",
      "testing feature:  var_23 72  out of  108\n",
      "New Test ROC AUC=0.7048756279576278\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00020522682692303285\n",
      "remove:  var_23\n",
      "\n",
      "testing feature:  var_20 73  out of  108\n",
      "New Test ROC AUC=0.7051229635392817\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=4.2108754730874054e-05\n",
      "remove:  var_20\n",
      "\n",
      "testing feature:  var_19 74  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_19\n",
      "\n",
      "testing feature:  var_13 75  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_13\n",
      "\n",
      "testing feature:  var_11 76  out of  108\n",
      "New Test ROC AUC=0.7050648551650296\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-1.5999619521256392e-05\n",
      "remove:  var_11\n",
      "\n",
      "testing feature:  var_10 77  out of  108\n",
      "New Test ROC AUC=0.705041014268563\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-3.9840515987821234e-05\n",
      "remove:  var_10\n",
      "\n",
      "testing feature:  var_8 78  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_8\n",
      "\n",
      "testing feature:  var_7 79  out of  108\n",
      "New Test ROC AUC=0.7050234293208881\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-5.742546366271917e-05\n",
      "remove:  var_7\n",
      "\n",
      "testing feature:  var_6 80  out of  108\n",
      "New Test ROC AUC=0.7049860521609547\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-9.480262359617964e-05\n",
      "remove:  var_6\n",
      "\n",
      "testing feature:  var_5 81  out of  108\n",
      "New Test ROC AUC=0.705058404098927\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-2.245068562389818e-05\n",
      "remove:  var_5\n",
      "\n",
      "testing feature:  var_4 82  out of  108\n",
      "New Test ROC AUC=0.7051045371482019\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=2.3682363651023053e-05\n",
      "remove:  var_4\n",
      "\n",
      "testing feature:  var_3 83  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_3\n",
      "\n",
      "testing feature:  var_35 84  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_35\n",
      "\n",
      "testing feature:  var_38 85  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_38\n",
      "\n",
      "testing feature:  var_63 86  out of  108\n",
      "New Test ROC AUC=0.7050936715529326\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=1.2816768381718191e-05\n",
      "remove:  var_63\n",
      "\n",
      "testing feature:  var_39 87  out of  108\n",
      "New Test ROC AUC=0.705040526475285\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-4.0328309265813544e-05\n",
      "remove:  var_39\n",
      "\n",
      "testing feature:  var_62 88  out of  108\n",
      "New Test ROC AUC=0.7051130003615769\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=3.21455770260215e-05\n",
      "remove:  var_62\n",
      "\n",
      "testing feature:  var_60 89  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_60\n",
      "\n",
      "testing feature:  var_59 90  out of  108\n",
      "New Test ROC AUC=0.7050339412660309\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-4.6913518519930975e-05\n",
      "remove:  var_59\n",
      "\n",
      "testing feature:  var_58 91  out of  108\n",
      "New Test ROC AUC=0.7051279512255502\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=4.709644099931687e-05\n",
      "remove:  var_58\n",
      "\n",
      "testing feature:  var_57 92  out of  108\n",
      "New Test ROC AUC=0.7050119539840212\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-6.890080052968095e-05\n",
      "remove:  var_57\n",
      "\n",
      "testing feature:  var_56 93  out of  108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Test ROC AUC=0.7050339412660309\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-4.6913518519930975e-05\n",
      "remove:  var_56\n",
      "\n",
      "testing feature:  var_2 94  out of  108\n",
      "New Test ROC AUC=0.7050808547845508\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=0.0\n",
      "remove:  var_2\n",
      "\n",
      "testing feature:  var_54 95  out of  108\n",
      "New Test ROC AUC=0.7047574112566835\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.0003234435278673953\n",
      "remove:  var_54\n",
      "\n",
      "testing feature:  var_52 96  out of  108\n",
      "New Test ROC AUC=0.7051184758411232\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=3.76210565723456e-05\n",
      "remove:  var_52\n",
      "\n",
      "testing feature:  var_51 97  out of  108\n",
      "New Test ROC AUC=0.704974259758457\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00010659502609389193\n",
      "remove:  var_51\n",
      "\n",
      "testing feature:  var_50 98  out of  108\n",
      "New Test ROC AUC=0.7049569796815808\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00012387510297007243\n",
      "remove:  var_50\n",
      "\n",
      "testing feature:  var_49 99  out of  108\n",
      "New Test ROC AUC=0.7050322583792215\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-4.859640532939302e-05\n",
      "remove:  var_49\n",
      "\n",
      "testing feature:  var_46 100  out of  108\n",
      "New Test ROC AUC=0.7050531481263557\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-2.7706658195181255e-05\n",
      "remove:  var_46\n",
      "\n",
      "testing feature:  var_45 101  out of  108\n",
      "New Test ROC AUC=0.7048302875724296\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-0.00025056721212124433\n",
      "remove:  var_45\n",
      "\n",
      "testing feature:  var_44 102  out of  108\n",
      "New Test ROC AUC=0.7050956349208768\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=1.478013632594255e-05\n",
      "remove:  var_44\n",
      "\n",
      "testing feature:  var_43 103  out of  108\n",
      "New Test ROC AUC=0.7050440995610471\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-3.675522350377047e-05\n",
      "remove:  var_43\n",
      "\n",
      "testing feature:  var_42 104  out of  108\n",
      "New Test ROC AUC=0.7050570016932524\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-2.3853091298486895e-05\n",
      "remove:  var_42\n",
      "\n",
      "testing feature:  var_41 105  out of  108\n",
      "New Test ROC AUC=0.7050512701222348\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-2.958466231606227e-05\n",
      "remove:  var_41\n",
      "\n",
      "testing feature:  var_40 106  out of  108\n",
      "New Test ROC AUC=0.7051034518081583\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=2.2597023607406896e-05\n",
      "remove:  var_40\n",
      "\n",
      "testing feature:  var_109 107  out of  108\n",
      "New Test ROC AUC=0.7050339412660309\n",
      "Previous round Test ROC AUC=0.7050808547845508\n",
      "Increase in ROC AUC=-4.6913518519930975e-05\n",
      "remove:  var_109\n",
      "DONE!!\n",
      "total features to keep:  9\n"
     ]
    }
   ],
   "source": [
    "# bước cuối cùng là lần lượt thêm một đặc trưng\n",
    "# từ quan trọng nhất tới ít quan trọng nhất, xây dựng mô hình\n",
    "# và xác định chất lượng\n",
    "\n",
    "# sau khi đã xây dựng mô hình, tính roc-auc mới\n",
    "# nếu roc-auc mới lớn hơn roc-auc ban đầu\n",
    "# (với 1 đặc trưng) thì đặc trưng được thêm \n",
    "# quan trọng, chúng ta cần giữ nó lại.\n",
    "# nếu không, hãy loại bỏ nó\n",
    "\n",
    "# loại bỏ đặc trưng đệ quy:\n",
    "\n",
    "# trước tiên chúng ta đặt tùy ý mức tăng trong roc-auc\n",
    "# nếu mức tăng vượt ngưỡng này,\n",
    "# thì giữ đặc trưng lại\n",
    "tol = 0.0001\n",
    "\n",
    "print('doing recursive feature addition')\n",
    "\n",
    "# hãy khởi tạo một danh sách thu thập\n",
    "# các đặc trưng nên giữ lại\n",
    "features_to_keep = [features[0]]\n",
    "\n",
    "# đặt bộ đếm để biết đặc trưng nào đang được đánh giá\n",
    "count = 1\n",
    "\n",
    "# giờ chúng ta lặp qua toàn bộ các đặc trưng, theo độ quan trọng:\n",
    "# nhớ rằng các đặc trưng trong danh sách được sắp xếp\n",
    "# theo độ quan trọng\n",
    "for feature in features[1:]:\n",
    "    print()\n",
    "    print('testing feature: ', feature, count, ' out of ', len(features))\n",
    "    count = count + 1\n",
    "\n",
    "    # khởi tạo mô hình\n",
    "    model_int = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "    # khớp mô hình với các đặc trưng đã chọn\n",
    "    # và đặc trưng cần đánh giá\n",
    "    model_int.fit(\n",
    "        X_train[features_to_keep + [feature] ], y_train)\n",
    "\n",
    "    # đưa ra dự đoán trên tập kiểm tra\n",
    "    ## Yêu cầu 23:\n",
    "    ## VIẾT CODE Ở ĐÂY:\n",
    "    y_pred_test = model_int.predict_proba(\n",
    "        X_test[features_to_keep + [feature] ])[:, 1]\n",
    "\n",
    "    # tính toán roc-auc mới\n",
    "    roc_int = roc_auc_score(y_test, y_pred_test)\n",
    "    print('New Test ROC AUC={}'.format((roc_int)))\n",
    "\n",
    "    # in ra roc-auc ban đầu với một đặc trưng\n",
    "    print('Previous round Test ROC AUC={}'.format((roc_first)))\n",
    "\n",
    "    # xác định mức tăng trong roc-auc\n",
    "    diff_roc = roc_int - roc_first\n",
    "\n",
    "    # so sánh mức tăng trong roc-auc với dung sai\n",
    "    # chúng ta đã thiết lập trước đó\n",
    "    if diff_roc >= tol:\n",
    "        print('Increase in ROC AUC={}'.format(diff_roc))\n",
    "        print('keep: ', feature)\n",
    "        print\n",
    "        # nếu mức tăng trong roc lớn hơn ngưỡng\n",
    "        # chúng ta giữ lại đặc trưng và điều chỉnh lại roc-auc cho giá trị mới\n",
    "        # xem xét đặc trưng đã thêm\n",
    "        roc_first = roc_int\n",
    "        \n",
    "        # và thêm đặc trưng cần giữ lại vào danh sách\n",
    "        features_to_keep.append(feature)\n",
    "    else:\n",
    "        # bỏ qua đặc trưng\n",
    "        print('Increase in ROC AUC={}'.format(diff_roc))\n",
    "        print('remove: ', feature)\n",
    "        print\n",
    "\n",
    "# vòng lặp giờ đã hoàn thành, hãy đánh giá toàn bộ đặc trưng\n",
    "print('DONE!!')\n",
    "print('total features to keep: ', len(features_to_keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Sjd5XOrwSNh"
   },
   "source": [
    "<details><summary> Gợi ý </summary>\n",
    "\n",
    "Sử dụng ```features_to_keep```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "qZh2tuoxrL_5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test selected features ROC AUC=0.705081\n"
     ]
    }
   ],
   "source": [
    "# cuối cùng, hãy kiểm tra chất lượng của mô hình đã xây sử dụng đặc trưng đã chọn\n",
    "# so với mô hình đầy đủ\n",
    "\n",
    "# xây dựng mô hình đầu tiên\n",
    "model_final = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "# khớp mô hình với các đặc trưng đã chọn\n",
    "model_final.fit(X_train[features_to_keep], y_train)\n",
    "\n",
    "# đưa ra dự đoán\n",
    "y_pred_test = model_final.predict_proba(X_test[features_to_keep])[:, 1]\n",
    "\n",
    "# tính roc-auc\n",
    "roc_final = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test selected features ROC AUC=%f' % (roc_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQZOwYDerTxY"
   },
   "source": [
    "Như các bạn thấy, mô hình Gradient Boosting với 15 đặc trưng cho thấy chất lượng tương tự với mô hình được xây với tập dữ liệu đầy đủ (cuộn lên trên để tìm giá trị này, chúng ta đã tính một vài cell trước đó).\n",
    "\n",
    "**Bài tập:** Hãy thử các giá trị dung sai khác. Thử với các ngưỡng nhỏ hơn hoặc lớn hơn để hiểu được tác động của điều này tới số lượng các đặc trưng được chọn."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[VN]11_1_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
